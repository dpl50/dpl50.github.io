WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.165
So, while we have paired sets of images,

00:00:03.165 --> 00:00:09.300
then we have [inaudible] truth input x and n target output y and we can create,

00:00:09.300 --> 00:00:13.695
again, to learn a mapping directly from x to y.

00:00:13.695 --> 00:00:19.714
In practice, parallel data is termed intensive and difficult to collect,

00:00:19.714 --> 00:00:22.769
often require huge amounts of manual labor.

00:00:22.769 --> 00:00:29.655
For example, there are only a few small datasets for semantic segmentation.

00:00:29.655 --> 00:00:34.018
In some scenarios, especially in stylized images,

00:00:34.018 --> 00:00:37.034
paired data is impossible to get.

00:00:37.034 --> 00:00:41.814
For example, if we would like to transform a horse to a zebra,

00:00:41.814 --> 00:00:46.679
we cannot ask a zebra to do the same pose with the same environment,

00:00:46.679 --> 00:00:49.519
or if you want to sell any image in

00:00:49.520 --> 00:00:53.680
a general style for Van Gogh rosagnal specific painting,

00:00:53.679 --> 00:00:58.310
we cannot ask Van Gog do a painting of any image.

00:00:58.310 --> 00:01:03.035
So, researchers have studied ways to learn from unpaired data.

00:01:03.034 --> 00:01:09.004
Instead, see we have a set of images of x and unpaired set y.

00:01:09.004 --> 00:01:14.435
We want to be able to translate an image from one set to another.

00:01:14.435 --> 00:01:20.325
We want to find a mapping G has tries its best to map x to y.

00:01:20.325 --> 00:01:26.915
With unpaired data, we no longer have the ability to look at real and fake pairs of data,

00:01:26.915 --> 00:01:29.645
as we did with peaks to peaks.

00:01:29.644 --> 00:01:32.599
But, we know that we can change

00:01:32.599 --> 00:01:37.789
our model to produce an output that belongs to a target domain.

00:01:37.790 --> 00:01:40.805
So, for when you push image of a horse,

00:01:40.805 --> 00:01:45.605
we can chain a generator to produce realistic looking images of zebras,

00:01:45.605 --> 00:01:48.090
so we these criteria.

00:01:48.090 --> 00:01:50.760
So, the problem with this, of course,

00:01:50.760 --> 00:01:55.630
is we count forced output of generator to corresponding to its input,

00:01:55.629 --> 00:01:59.454
and we end up with a problem called mode collapse,

00:01:59.454 --> 00:02:05.344
in which a model might map multiple input horse into the same zebra.

00:02:05.344 --> 00:02:09.514
In such cases, giving a input horse,

00:02:09.514 --> 00:02:13.789
all we know is that the output should look like a zebra,

00:02:13.789 --> 00:02:18.125
and we can do this using an adversarial loss on the output.

00:02:18.125 --> 00:02:20.659
But, is this enough?

00:02:20.659 --> 00:02:22.984
There are many, many mappings.

00:02:22.985 --> 00:02:26.215
We'll satisfy such an unparalleled constraint.

00:02:26.215 --> 00:02:33.965
So, we need another constraint to make sure G is in d to the correct mapping.

00:02:33.965 --> 00:02:36.185
We add an additional mapping,

00:02:36.185 --> 00:02:41.194
subtract validity of G. This has an inverse mapping,

00:02:41.194 --> 00:02:46.655
chord G inverse has tries to map y to x.

00:02:46.655 --> 00:02:50.800
This is called cycle consistency constraint.

00:02:50.800 --> 00:02:53.175
If you think of it like this,

00:02:53.175 --> 00:02:57.305
if we translate a sentence from English to French,

00:02:57.305 --> 00:03:00.920
and then we translate back from French to English,

00:03:00.919 --> 00:03:03.744
we should arrive back at the original sentence.

00:03:03.745 --> 00:03:08.974
A complete translation cycle should bring back to the original sentence,

00:03:08.974 --> 00:03:12.784
and it's the same for transforming images.

00:03:12.784 --> 00:03:15.694
If we translate a horse to a zebra,

00:03:15.694 --> 00:03:17.724
and then trace it back,

00:03:17.724 --> 00:03:21.919
we shall get the same horse and we started with.

00:03:21.919 --> 00:03:27.004
A one these two mappings to access checks of one another.

00:03:27.004 --> 00:03:30.185
If both mappings are satisfied,

00:03:30.185 --> 00:03:37.625
then T-inverse applied to G of x should be indistinguishable from the original image x.

00:03:37.625 --> 00:03:43.564
Next, let's discuss how we can implement cycle consensus laws,

00:03:43.564 --> 00:03:47.639
and you water the training process looks like for cycle again.

