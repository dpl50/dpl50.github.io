WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.790
So, in the Pix2Pix architecture,

00:00:02.790 --> 00:00:08.219
we need a way to associate an input image with its correct output image.

00:00:08.220 --> 00:00:10.425
So, to measure input and output,

00:00:10.425 --> 00:00:13.110
we can modify the discriminator,

00:00:13.109 --> 00:00:17.714
so that instead of identifying a single image as real or fake,

00:00:17.714 --> 00:00:21.643
it will look at pairs of input and output images

00:00:21.643 --> 00:00:26.309
and output a value for a real pair or a fake pair.

00:00:26.309 --> 00:00:29.924
This is where the pair of training data comes in.

00:00:29.925 --> 00:00:36.299
During training, the discriminator will take an input image and the unknown image,

00:00:36.299 --> 00:00:41.479
that is either the target image y or generate image g of x.

00:00:41.479 --> 00:00:44.494
By labeling a pair as fake or real,

00:00:44.494 --> 00:00:50.149
it tries to determine whether the output image is generated or not.

00:00:50.149 --> 00:00:54.679
Though the weights of the discriminator are then adjusted based on

00:00:54.679 --> 00:00:59.179
the cascading error of the input or generated output pair,

00:00:59.179 --> 00:01:03.204
which the discriminator tries to classify as force.

00:01:03.204 --> 00:01:06.284
The input to target pair.

00:01:06.284 --> 00:01:09.575
What should discriminator tries to classify as real?

00:01:09.575 --> 00:01:15.290
The generator's weights are also adjusted based on the output of a discriminator.

00:01:15.290 --> 00:01:17.000
Only in this case,

00:01:17.000 --> 00:01:22.180
the generator wants the classification error to be large it wants the input or

00:01:22.180 --> 00:01:28.745
generated output pair to be classified as real and again was a discriminator gets better,

00:01:28.745 --> 00:01:30.364
so does the generator.

00:01:30.364 --> 00:01:34.310
The discriminatory is acting as the loss function and is

00:01:34.310 --> 00:01:39.769
conditional on both the input and output images to the generator,

00:01:39.769 --> 00:01:43.104
which is why this is called conditional gain.

00:01:43.105 --> 00:01:46.400
The nice thing about pix2pix is that it makes

00:01:46.400 --> 00:01:51.215
no assumption about the relationship between pairs of images.

00:01:51.215 --> 00:01:55.945
Instead, it's lensed object a functioning as it shrinks,

00:01:55.944 --> 00:02:00.424
and compares the defined inputs and outputs during shrinking.

00:02:00.424 --> 00:02:06.024
A trained pix2pix architecture has been shown to learn a variety of mappings.

00:02:06.025 --> 00:02:11.750
From Google Maps to satellite images to automatically recurring images.

00:02:11.750 --> 00:02:15.759
You can even check out a pix2pix demo made by Chris,

00:02:15.759 --> 00:02:18.639
has transformed sketches into cats.

00:02:18.639 --> 00:02:22.429
You can get some interesting and kind of creepy results

00:02:22.430 --> 00:02:27.665
and it may give you some insights as to what this model is really good at listening.

00:02:27.664 --> 00:02:31.534
I encourage you to check out the link to the demo below.

00:02:31.534 --> 00:02:36.740
Now, you may be wondering is the other way to learn how to do image to

00:02:36.740 --> 00:02:43.189
image translation without his nicely pair of training images and there it is.

00:02:43.189 --> 00:02:46.669
Next, we will talk about an architecture that

00:02:46.669 --> 00:02:50.280
can learned for unpaired data, the cycle again.

