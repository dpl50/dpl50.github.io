<!-- udacimak v1.4.4 -->
<!DOCTYPE html>
<html lang="en">
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <meta content="ie=edge" http-equiv="X-UA-Compatible"/>
  <title>
   Sigmoid Function
  </title>
  <link href="../assets/css/bootstrap.min.css" rel="stylesheet"/>
  <link href="../assets/css/plyr.css" rel="stylesheet"/>
  <link href="../assets/css/katex.min.css" rel="stylesheet"/>
  <link href="../assets/css/jquery.mCustomScrollbar.min.css" rel="stylesheet"/>
  <link href="../assets/css/styles.css" rel="stylesheet"/>
  <link href="../assets/img/udacimak.png" rel="shortcut icon" type="image/png">
  </link>
 </head>
 <body>
  <div class="wrapper">
   <nav id="sidebar">
    <div class="sidebar-header">
     <h3>
      MiniFlow
     </h3>
    </div>
    <ul class="sidebar-list list-unstyled CTAs">
     <li>
      <a class="article" href="../index.html">
       Back to Home
      </a>
     </li>
    </ul>
    <ul class="sidebar-list list-unstyled components">
     <li class="">
      <a href="01. Welcome to MiniFlow.html">
       01. Welcome to MiniFlow
      </a>
     </li>
     <li class="">
      <a href="02. Graphs.html">
       02. Graphs
      </a>
     </li>
     <li class="">
      <a href="03. MiniFlow Architecture.html">
       03. MiniFlow Architecture
      </a>
     </li>
     <li class="">
      <a href="04. Forward Propagation.html">
       04. Forward Propagation
      </a>
     </li>
     <li class="">
      <a href="05. Forward Propagation Solution.html">
       05. Forward Propagation Solution
      </a>
     </li>
     <li class="">
      <a href="06. Learning and Loss.html">
       06. Learning and Loss
      </a>
     </li>
     <li class="">
      <a href="07. Linear Transform.html">
       07. Linear Transform
      </a>
     </li>
     <li class="">
      <a href="08. Sigmoid Function.html">
       08. Sigmoid Function
      </a>
     </li>
     <li class="">
      <a href="09. Cost.html">
       09. Cost
      </a>
     </li>
     <li class="">
      <a href="10. Cost Solution.html">
       10. Cost Solution
      </a>
     </li>
     <li class="">
      <a href="11. Gradient Descent.html">
       11. Gradient Descent
      </a>
     </li>
     <li class="">
      <a href="12. Backpropagation.html">
       12. Backpropagation
      </a>
     </li>
     <li class="">
      <a href="13. Stochastic Gradient Descent.html">
       13. Stochastic Gradient Descent
      </a>
     </li>
     <li class="">
      <a href="14. SGD Solution.html">
       14. SGD Solution
      </a>
     </li>
     <li class="">
      <a href="15. Outro.html">
       15. Outro
      </a>
     </li>
    </ul>
    <ul class="sidebar-list list-unstyled CTAs">
     <li>
      <a class="article" href="../index.html">
       Back to Home
      </a>
     </li>
    </ul>
   </nav>
   <div id="content">
    <header class="container-fluild header">
     <div class="container">
      <div class="row">
       <div class="col-12">
        <div class="align-items-middle">
         <button class="btn btn-toggle-sidebar" id="sidebarCollapse" type="button">
          <div>
          </div>
          <div>
          </div>
          <div>
          </div>
         </button>
         <h1 style="display: inline-block">
          08. Sigmoid Function
         </h1>
        </div>
       </div>
      </div>
     </div>
    </header>
    <main class="container">
     <div class="row">
      <div class="col-12">
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          Here's my solution to the last quiz:
         </p>
         <pre><code>class Linear(Node):
    def __init__(self, X, W, b):
        # Notice the ordering of the inputs passed to the
        # Node constructor.
        Node.__init__(self, [X, W, b])

    def forward(self):
        X = self.inbound_nodes[0].value
        W = self.inbound_nodes[1].value
        b = self.inbound_nodes[2].value
        self.value = np.dot(X, W) + b</code></pre>
         <p>
          Nothing fancy in my solution. I pulled the value of the
          <code>
           X
          </code>
          ,
          <code>
           W
          </code>
          and
          <code>
           b
          </code>
          from their respective inputs. I used
          <code>
           np.dot
          </code>
          to handle the matrix multiplication.
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <h3 id="sigmoid-function">
          Sigmoid Function
         </h3>
         <p>
          Neural networks take advantage of alternating transforms and activation functions to better categorize outputs. The sigmoid function is among the most common activation functions.
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <figure class="figure">
          <img alt="Equation (3)" class="img img-fluid" src="img/19.png"/>
          <figcaption class="figure-caption">
           <p>
            Equation (3)
           </p>
          </figcaption>
         </figure>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <figure class="figure">
          <img alt='Graph of the sigmoid function. Notice the "S" shape.' class="img img-fluid" src="img/pasted-image-at-2016-10-25-01-17-pm.png"/>
          <figcaption class="figure-caption">
           <p>
            Graph of the sigmoid function. Notice the "S" shape.
           </p>
          </figcaption>
         </figure>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          Linear transforms are great for simply
          <em>
           shifting
          </em>
          values, but neural networks often require a more nuanced transform. For instance, one of the original designs for an artificial neuron,
          <a href="https://en.wikipedia.org/wiki/Perceptron" rel="noopener noreferrer" target="_blank">
           the perceptron
          </a>
          , exhibits binary output behavior. Perceptrons compare a weighted input to a threshold. When the weighted input exceeds the threshold, the perceptron is
          <strong>
           activated
          </strong>
          and outputs
          <code>
           1
          </code>
          , otherwise it outputs
          <code>
           0
          </code>
          .
         </p>
         <p>
          You could model a perceptron's behavior as a step function:
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <figure class="figure">
          <img alt="Example of a step function (The jump between y = 0 and y = 1 should be instantaneous)." class="img img-fluid" src="img/save-2.png"/>
          <figcaption class="figure-caption">
           <p>
            Example of a step function (The jump between y = 0 and y = 1 should be instantaneous).
           </p>
          </figcaption>
         </figure>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          Activation, the idea of binary output behavior, generally makes sense for classification problems. For example, if you ask the network to hypothesize if a handwritten image is a '9', you're effectively asking for a binary output -
          <em>
           yes
          </em>
          , this is a '9', or
          <em>
           no
          </em>
          , this is not a '9'. A step function is the starkest form of a binary output, which is great, but step functions are not continuous and not differentiable, which is
          <em>
           very bad
          </em>
          . Differentiation is what makes gradient descent possible.
         </p>
         <p>
          The sigmoid function, Equation (3) above, replaces thresholding with a beautiful S-shaped curve (also shown above) that mimics the activation behavior of a perceptron while being differentiable. As a bonus, the sigmoid function has a very simple derivative that that can be calculated from the sigmoid function itself, as shown in Equation (4) below.
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <figure class="figure">
          <img alt="Equation (4). &lt;span class='mathquill'&gt;\large \sigma&lt;/span&gt; represents Equation (3), the sigmoid function." class="img img-fluid" src="img/21.png"/>
          <figcaption class="figure-caption">
           <p>
            Equation (4).
            <span class="mathquill ud-math">
             \large \sigma
            </span>
            represents Equation (3), the sigmoid function.
           </p>
          </figcaption>
         </figure>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          Notice that the sigmoid function only has one parameter. Remember that sigmoid is an
          <em>
           activation
          </em>
          function (
          <em>
           non-linearity
          </em>
          ), meaning it takes a single input and performs a mathematical operation on it.
         </p>
         <p>
          Conceptually, the sigmoid function makes decisions. When given weighted features from some data, it indicates whether or not the features contribute to a classification. In that way, a sigmoid activation works well following a linear transformation. As it stands right now with random weights and bias, the sigmoid node's output is also random. The process of learning through backpropagation and gradient descent, which you will implement soon, modifies the weights and bias such that activation of the sigmoid node begins to match expected outputs.
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          Now that I've given you the equation for the sigmoid function, I want you to add it to the
          <code>
           MiniFlow
          </code>
          library. To do so, you'll want to use
          <code>
           np.exp
          </code>
          (
          <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.exp.html" rel="noopener noreferrer" target="_blank">
           documentation
          </a>
          ) to make your life much easier.
         </p>
         <p>
          You'll be using
          <code>
           Sigmoid
          </code>
          in conjunction with
          <code>
           Linear
          </code>
          . Here's how it should look:
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <figure class="figure">
          <img alt="Inputs &gt; Linear Transform &gt; Sigmoid" class="img img-fluid" src="img/screen-shot-2016-10-26-at-19.28.34.png"/>
          <figcaption class="figure-caption">
           <p>
            Inputs &gt; Linear Transform &gt; Sigmoid
           </p>
          </figcaption>
         </figure>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <h3 id="instructions">
          Instructions
         </h3>
         <ol>
          <li>
           Open nn.py to see how the network will use
           <code>
            Sigmoid
           </code>
           .
          </li>
          <li>
           Open miniflow.py. Modify the
           <code>
            forward
           </code>
           method of the
           <code>
            Sigmoid
           </code>
           class to reflect the sigmoid function's behavior.
          </li>
          <li>
           Test your work! Hit "Submit" when your
           <code>
            Sigmoid
           </code>
           works as expected.
          </li>
         </ol>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <h4>
          Start Quiz:
         </h4>
         <div>
          <div class="nav nav-tabs nav-fill" id="question-tabs" role="tablist">
           <a aria-controls="197340-nn-py" aria-selected="true" class="nav-item nav-link active show" data-toggle="tab" href="#197340-nn-py" id="tab-197340-nn-py" role="tab">
            nn.py
           </a>
           <a aria-controls="197340-miniflow-py" aria-selected="false" class="nav-item nav-link" data-toggle="tab" href="#197340-miniflow-py" id="tab-197340-miniflow-py" role="tab">
            miniflow.py
           </a>
          </div>
          <div class="tab-content" id="question-tab-contents" style="padding: 20px 0;">
           <div aria-labelledby="tab-197340-nn-py" class="tab-pane active show" id="197340-nn-py" role="tabpanel">
            <pre><code></code>"""
This network feeds the output of a linear transform
to the sigmoid function.

Finish implementing the Sigmoid class in miniflow.py!

Feel free to play around with this network, too!
"""

import numpy as np
from miniflow import *

X, W, b = Input(), Input(), Input()

f = Linear(X, W, b)
g = Sigmoid(f)

X_ = np.array([[-1., -2.], [-1, -2]])
W_ = np.array([[2., -3], [2., -3]])
b_ = np.array([-3., -5])

feed_dict = {X: X_, W: W_, b: b_}

graph = topological_sort(feed_dict)
output = forward_pass(g, graph)

"""
Output should be:
[[  1.23394576e-04   9.82013790e-01]
 [  1.23394576e-04   9.82013790e-01]]
"""
print(output)
</pre>
           </div>
           <div aria-labelledby="tab-197340-miniflow-py" class="tab-pane" id="197340-miniflow-py" role="tabpanel">
            <pre><code></code>"""
Fix the Sigmoid class so that it computes the sigmoid function
on the forward pass!

Scroll down to get started.
"""

import numpy as np

class Node(object):
    def __init__(self, inbound_nodes=[]):
        self.inbound_nodes = inbound_nodes
        self.value = None
        self.outbound_nodes = []
        for node in inbound_nodes:
            node.outbound_nodes.append(self)

    def forward():
        raise NotImplementedError


class Input(Node):
    def __init__(self):
        # An Input node has no inbound nodes,
        # so no need to pass anything to the Node instantiator
        Node.__init__(self)

    def forward(self):
        # Do nothing because nothing is calculated.
        pass


class Linear(Node):
    def __init__(self, X, W, b):
        # Notice the ordering of the input nodes passed to the
        # Node constructor.
        Node.__init__(self, [X, W, b])

    def forward(self):
        X = self.inbound_nodes[0].value
        W = self.inbound_nodes[1].value
        b = self.inbound_nodes[2].value
        self.value = np.dot(X, W) + b


class Sigmoid(Node):
    """
    You need to fix the `_sigmoid` and `forward` methods.
    """
    def __init__(self, node):
        Node.__init__(self, [node])

    def _sigmoid(self, x):
        """
        This method is separate from `forward` because it
        will be used later with `backward` as well.

        `x`: A numpy array-like object.

        Return the result of the sigmoid function.

        Your code here!
        """


    def forward(self):
        """
        Set the value of this node to the result of the
        sigmoid function, `_sigmoid`.

        Your code here!
        """
        # This is a dummy value to prevent numpy errors
        # if you test without changing this method.
        self.value = -1


def topological_sort(feed_dict):
    """
    Sort the nodes in topological order using Kahn's Algorithm.

    `feed_dict`: A dictionary where the key is a `Input` Node and the value is the respective value feed to that Node.

    Returns a list of sorted nodes.
    """

    input_nodes = [n for n in feed_dict.keys()]

    G = {}
    nodes = [n for n in input_nodes]
    while len(nodes) &gt; 0:
        n = nodes.pop(0)
        if n not in G:
            G[n] = {'in': set(), 'out': set()}
        for m in n.outbound_nodes:
            if m not in G:
                G[m] = {'in': set(), 'out': set()}
            G[n]['out'].add(m)
            G[m]['in'].add(n)
            nodes.append(m)

    L = []
    S = set(input_nodes)
    while len(S) &gt; 0:
        n = S.pop()

        if isinstance(n, Input):
            n.value = feed_dict[n]

        L.append(n)
        for m in n.outbound_nodes:
            G[n]['out'].remove(m)
            G[m]['in'].remove(n)
            # if no other incoming edges add to S
            if len(G[m]['in']) == 0:
                S.add(m)
    return L


def forward_pass(output_node, sorted_nodes):
    """
    Performs a forward pass through a list of sorted Nodes.

    Arguments:

        `output_node`: A Node in the graph, should be the output node (have no outgoing edges).
        `sorted_nodes`: a topologically sorted list of nodes.

    Returns the output node's value
    """

    for n in sorted_nodes:
        n.forward()

    return output_node.value
</pre>
           </div>
          </div>
         </div>
        </div>
       </div>
       <div class="divider">
       </div>
      </div>
      <div class="col-12">
       <p class="text-right">
        <a class="btn btn-outline-primary mt-4" href="09. Cost.html" role="button">
         Next Concept
        </a>
       </p>
      </div>
     </div>
    </main>
    <footer class="footer">
     <div class="container">
      <div class="row">
       <div class="col-12">
        <p class="text-center">
         udacity2.0 If you need the newest courses Plase add me wechat: udacity6
        </p>
       </div>
      </div>
     </div>
    </footer>
   </div>
  </div>
  <script src="../assets/js/jquery-3.3.1.min.js">
  </script>
  <script src="../assets/js/plyr.polyfilled.min.js">
  </script>
  <script src="../assets/js/bootstrap.min.js">
  </script>
  <script src="../assets/js/jquery.mCustomScrollbar.concat.min.js">
  </script>
  <script src="../assets/js/katex.min.js">
  </script>
  <script>
   // Initialize Plyr video players
    const players = Array.from(document.querySelectorAll('video')).map(p => new Plyr(p));

    // render math equations
    let elMath = document.getElementsByClassName('mathquill');
    for (let i = 0, len = elMath.length; i < len; i += 1) {
      const el = elMath[i];

      katex.render(el.textContent, el, {
        throwOnError: false
      });
    }

    // this hack will make sure Bootstrap tabs work when using Handlebars
    if ($('#question-tabs').length && $('#user-answer-tabs').length) {
      $("#question-tabs a.nav-link").on('click', function () {
        $("#question-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
      $("#user-answer-tabs a.nav-link").on('click', function () {
        $("#user-answer-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
    } else {
      $("a.nav-link").on('click', function () {
        $(".tab-pane").hide();
        $($(this).attr("href")).show();
      });
    }

    // side bar events
    $(document).ready(function () {
      $("#sidebar").mCustomScrollbar({
        theme: "minimal"
      });

      $('#sidebarCollapse').on('click', function () {
        $('#sidebar, #content').toggleClass('active');
        $('.collapse.in').toggleClass('in');
        $('a[aria-expanded=true]').attr('aria-expanded', 'false');
      });

      // scroll to first video on page loading
      if ($('video').length) {
        $('html,body').animate({ scrollTop: $('div.plyr').prev().offset().top});
      }

      // auto play first video: this may not work with chrome/safari due to autoplay policy
      if (players && players.length > 0) {
        players[0].play();
      }

      // scroll sidebar to current concept
      const currentInSideBar = $( "ul.sidebar-list.components li a:contains('08. Sigmoid Function')" )
      currentInSideBar.css( "text-decoration", "underline" );
      $("#sidebar").mCustomScrollbar('scrollTo', currentInSideBar);
    });
  </script>
 </body>
</html>
