WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.240
Here's my complete solution for the discriminator class.

00:00:03.240 --> 00:00:06.089
At a quick glance, you can see that this is just a series

00:00:06.089 --> 00:00:08.949
of five convolutional layers and some BatchNorm.

00:00:08.949 --> 00:00:12.329
The first layer will take in an RGB image as input,

00:00:12.330 --> 00:00:14.460
and so the input depth is three,

00:00:14.460 --> 00:00:17.370
and then this creates a convolutional layer with a depth

00:00:17.370 --> 00:00:20.670
of conv_dim or a default value of 64.

00:00:20.670 --> 00:00:22.755
I'm also using a kernel size of four,

00:00:22.754 --> 00:00:24.704
and since this is the first layer,

00:00:24.704 --> 00:00:27.224
I'm setting batch_norm to false. All right.

00:00:27.225 --> 00:00:30.495
Then, we have a few layers that do have batch normalization applied,

00:00:30.495 --> 00:00:33.439
and these are just normal strided convolutional layers that

00:00:33.439 --> 00:00:37.509
increase the depth of the outputs from 64 to 128,

00:00:37.509 --> 00:00:40.244
and then from 128 to 256,

00:00:40.244 --> 00:00:44.509
and finally to a depth of 512 or conv_dim times 8.

00:00:44.509 --> 00:00:46.564
As an image passes through these layers,

00:00:46.564 --> 00:00:49.159
I also know that they should down-sample the x and y

00:00:49.159 --> 00:00:51.949
dimensions of any image by a factor of two.

00:00:51.950 --> 00:00:55.390
So, I've actually written the dimensions of each output here.

00:00:55.390 --> 00:00:57.024
By the end of layer four,

00:00:57.024 --> 00:00:59.659
all have gone from a 128 by 28 by

00:00:59.659 --> 00:01:04.879
3 input image down-sampled to get an 8 by 8 by 512 output.

00:01:04.879 --> 00:01:07.144
Then, we reach the last convolutional layer,

00:01:07.144 --> 00:01:10.549
conv5, which is just going to be our classification layer.

00:01:10.549 --> 00:01:14.719
This is a normal not strided convolutional layer that will take in

00:01:14.719 --> 00:01:17.209
our conv_dim times eight number of outputs from

00:01:17.209 --> 00:01:20.419
the previous layer and produce one value as output.

00:01:20.420 --> 00:01:24.844
This will be the discriminator logic that indicates whether an image is real or fake.

00:01:24.844 --> 00:01:27.829
To indicate that this is not a strided convolutional layer,

00:01:27.829 --> 00:01:29.420
I'll have a stride equal to one,

00:01:29.420 --> 00:01:30.754
a padding equal to one,

00:01:30.754 --> 00:01:32.560
and batch_norm set to false.

00:01:32.560 --> 00:01:35.004
Instead of using our helper conv function here,

00:01:35.004 --> 00:01:38.884
you could have also just used nn.conv2d to create this layer.

00:01:38.885 --> 00:01:41.675
As always, this is just one possible solution.

00:01:41.674 --> 00:01:43.554
So that completes the unit function,

00:01:43.555 --> 00:01:45.180
and then in the forward function,

00:01:45.180 --> 00:01:49.335
I'm just passing an input x through the convolutional layers in sequence.

00:01:49.334 --> 00:01:51.704
I'm passing x through our first convolutional layer,

00:01:51.704 --> 00:01:54.554
applying a ReLu activation and getting some output,

00:01:54.555 --> 00:01:56.670
then I'm passing this output to our next layer,

00:01:56.670 --> 00:01:58.215
and next, and next.

00:01:58.215 --> 00:02:02.060
Applying ReLu activations to all but the last convolutional layer,

00:02:02.060 --> 00:02:05.704
and that's it, a deep convolutional discriminator class.

00:02:05.704 --> 00:02:08.875
This completes one of our types of adversarial networks.

00:02:08.875 --> 00:02:11.129
Next, let's talk about the generator.

