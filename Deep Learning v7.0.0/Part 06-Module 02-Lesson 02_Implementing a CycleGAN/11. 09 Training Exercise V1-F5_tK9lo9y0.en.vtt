WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:01.544
To train our CycleGAN,

00:00:01.544 --> 00:00:04.695
we'll train our discriminators first and then the generators.

00:00:04.695 --> 00:00:07.035
Let's take D_X as an example.

00:00:07.035 --> 00:00:10.380
We'll first see how this performs on classifying real images.

00:00:10.380 --> 00:00:14.519
Then, generate some fake images and compute the fake loss for D_X.

00:00:14.519 --> 00:00:18.390
Then, we'll add up the real and fake loss components to get

00:00:18.390 --> 00:00:23.399
the total D_X loss and perform backpropagation and optimization as usual.

00:00:23.399 --> 00:00:26.489
Then, you just have to repeat these steps only applying them

00:00:26.489 --> 00:00:29.759
to train D_Y on real and fake Y images.

00:00:29.760 --> 00:00:31.800
When you get to the code, don't forget to

00:00:31.800 --> 00:00:35.564
zero-out your accumulated optimizer gradients at the start of training.

00:00:35.564 --> 00:00:38.144
Then, you'll move on to training the generators.

00:00:38.145 --> 00:00:43.925
Our generators take in real images as input and create generated fake images as output.

00:00:43.924 --> 00:00:47.784
So, as an example, let's start with our generator G_Y to X.

00:00:47.784 --> 00:00:49.334
As a first step for training,

00:00:49.335 --> 00:00:52.730
we'll generate images that look like they've come from domain X,

00:00:52.729 --> 00:00:55.339
based on real images in domain Y.

00:00:55.340 --> 00:00:58.450
So, we'll have fake X generated images.

00:00:58.450 --> 00:01:01.520
Then, you can compute the generators loss based on how

00:01:01.520 --> 00:01:05.030
our discriminator D_X responds to this fake X.

00:01:05.030 --> 00:01:08.870
Recall that our generator will have an adversarial loss function,

00:01:08.870 --> 00:01:11.120
which means that it wants its fake X to be

00:01:11.120 --> 00:01:14.880
classified as real images by our discriminator D_X.

00:01:14.879 --> 00:01:18.069
After passing this fake X to our discriminator, next,

00:01:18.069 --> 00:01:22.489
you'll calculate the reconstruction loss or cycle consistency loss.

00:01:22.489 --> 00:01:25.099
You'll generate reconstructed Y images

00:01:25.099 --> 00:01:28.204
based on the fake images generated in the first step.

00:01:28.204 --> 00:01:30.439
So, you'll be passing these fake X through

00:01:30.439 --> 00:01:33.709
our other generator to create a reconstructed Y.

00:01:33.709 --> 00:01:36.769
Then, you can calculate the cycle consistency loss by

00:01:36.769 --> 00:01:40.319
comparing the reconstructions with the real Y images.

00:01:40.319 --> 00:01:43.219
At this point, you'll also have to decide on a value for

00:01:43.219 --> 00:01:46.715
the Lambda weight parameter in our cycle consistency loss function.

00:01:46.715 --> 00:01:48.799
Then, you'll repeat this whole process doing

00:01:48.799 --> 00:01:51.679
the same thing only for our generator G_X to Y.

00:01:51.680 --> 00:01:57.530
You'll calculate its adversarial loss and we'll get real X and reconstructed X images.

00:01:57.530 --> 00:02:00.334
Finally, you'll add up all of these losses,

00:02:00.334 --> 00:02:05.245
so two adversarial generator losses and two reconstruction losses.

00:02:05.245 --> 00:02:07.954
This will give you your total generator loss.

00:02:07.954 --> 00:02:11.889
Then, you can perform backpropagation and optimization as usual.

00:02:11.889 --> 00:02:16.324
Now, all of these steps are also described in comments in the training loop,

00:02:16.324 --> 00:02:19.969
so you'll know which steps to complete in this training exercise.

00:02:19.969 --> 00:02:21.409
Below this long cell,

00:02:21.409 --> 00:02:23.659
you'll also see some tips for training.

00:02:23.659 --> 00:02:26.900
Basically, since these networks are acting as adversaries,

00:02:26.900 --> 00:02:30.099
you should see that there's always some level of discriminator loss.

00:02:30.099 --> 00:02:34.299
We'll print out the losses for each discriminator and the total generator loss.

00:02:34.300 --> 00:02:36.890
The generator loss is expected to start out much

00:02:36.889 --> 00:02:40.399
higher since it's the larger weighted summation of terms,

00:02:40.400 --> 00:02:43.835
but it should also see the biggest decrease at the start of training.

00:02:43.835 --> 00:02:46.445
If your loss isn't decreasing as you expect,

00:02:46.444 --> 00:02:48.064
or if it's jumping around a lot,

00:02:48.064 --> 00:02:49.789
this could mean that it's time to tweak

00:02:49.789 --> 00:02:53.870
your hyperparameters or change how you're weighting and calculating loss terms,

00:02:53.870 --> 00:02:56.390
and you always have the solution code to consult.

00:02:56.389 --> 00:03:00.174
I should also note that CycleGANs take quite awhile to train,

00:03:00.175 --> 00:03:04.160
often overnight or for several hours for larger images.

00:03:04.159 --> 00:03:07.879
In this case, I recommend training for about 1,000 iterations or

00:03:07.879 --> 00:03:11.900
less to get results that allow you to see the power of this kind of model,

00:03:11.900 --> 00:03:13.659
but that are not perfect.

00:03:13.659 --> 00:03:17.750
So, our goal is not to create perfect transformations, instead,

00:03:17.750 --> 00:03:20.719
it's really to visualize what this unsupervised approach has

00:03:20.719 --> 00:03:24.000
learned about the distribution of summer and winter images.

