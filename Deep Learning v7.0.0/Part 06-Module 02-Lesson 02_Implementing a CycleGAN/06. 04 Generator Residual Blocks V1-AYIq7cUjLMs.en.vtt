WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:06.075
The generators G_XtoY and G_YtoX take in one kind of image as input.

00:00:06.075 --> 00:00:10.620
For example, G_XtoY should take in an image from the X domain

00:00:10.619 --> 00:00:15.074
and it will output a transformed image that looks like it belongs to the other domain,

00:00:15.074 --> 00:00:16.859
in this case, the Y domain.

00:00:16.859 --> 00:00:20.114
These generators look a bit like auto-encoders.

00:00:20.114 --> 00:00:22.034
They're made of an encoder portion,

00:00:22.035 --> 00:00:25.649
a convolutional net that's responsible for training an image into

00:00:25.649 --> 00:00:29.534
a smaller feature representation and a decoder portion,

00:00:29.535 --> 00:00:32.450
a transpose-convolutional net that's responsible for taking

00:00:32.450 --> 00:00:36.054
a smaller representation and turning it into a transformed image.

00:00:36.054 --> 00:00:38.179
There's one big difference here though and that's

00:00:38.179 --> 00:00:41.164
in-between the encoder and decoder portions.

00:00:41.164 --> 00:00:44.884
In-between these two portions are a series of residual blocks,

00:00:44.884 --> 00:00:50.034
usually between six and nine or even more residual blocks for larger images.

00:00:50.034 --> 00:00:53.989
This is what we'll discuss before talking about this complete architecture.

00:00:53.990 --> 00:00:58.085
Since these residual blocks are a new piece in this CycleGAN generator,

00:00:58.085 --> 00:01:00.859
I want to spend some time explaining what they are and

00:01:00.859 --> 00:01:03.820
how they work before talking about the generator as a whole.

00:01:03.820 --> 00:01:05.269
To complete these blocks,

00:01:05.269 --> 00:01:08.454
I'll be asking you to define a residual block class.

00:01:08.454 --> 00:01:12.109
It may help to think about where you might have heard the term residual before.

00:01:12.109 --> 00:01:14.480
In the lesson on convolutional neural networks,

00:01:14.480 --> 00:01:18.500
we briefly mentioned the Resnet-50 architecture for image classification,

00:01:18.500 --> 00:01:19.840
which looks like this.

00:01:19.840 --> 00:01:22.670
It's a series of convolutional layers and you can

00:01:22.670 --> 00:01:25.609
see these loop like connections between a couple of layers.

00:01:25.609 --> 00:01:30.349
These two layers where the input and output are connected are called Resnet blocks.

00:01:30.349 --> 00:01:34.084
These blocks are made of layers that have the same number of inputs as

00:01:34.084 --> 00:01:39.219
outputs and they sum up the output of one layer with the input of a previous layer.

00:01:39.219 --> 00:01:41.599
The motivation for this kind of structure is that

00:01:41.599 --> 00:01:44.449
researchers found a problem with very deep networks.

00:01:44.450 --> 00:01:47.030
During training, they would see that deeper networks would

00:01:47.030 --> 00:01:50.135
stop learning at some point and wouldn't reach convergence.

00:01:50.135 --> 00:01:52.975
This is sometimes called training degradation.

00:01:52.974 --> 00:01:56.584
Essentially, the training accuracy would get saturated at some point

00:01:56.584 --> 00:02:00.679
and then the network would either never improve or even become worse.

00:02:00.680 --> 00:02:04.520
One solution to this problem is to use resonant blocks that are allowed to

00:02:04.519 --> 00:02:08.569
learn so-called residual functions as they're applied to layer inputs.

00:02:08.569 --> 00:02:11.389
You can read more about this proposed architecture in

00:02:11.389 --> 00:02:15.519
the paper Deep Residual Learning for Image Recognition, which I've linked here.

00:02:15.520 --> 00:02:17.219
Now, usually in a model,

00:02:17.219 --> 00:02:21.754
we'll have one layer that looks at an input X and directly produces an output Y.

00:02:21.754 --> 00:02:24.560
With Resnet blocks, the idea is that the end of

00:02:24.560 --> 00:02:28.534
a deep layer we can add an additional layer too like these ones pictured.

00:02:28.534 --> 00:02:30.859
These layers will have the same number of outputs as

00:02:30.860 --> 00:02:34.190
inputs and so they're not changing the shape of the input data.

00:02:34.189 --> 00:02:40.194
The key behavior is that a Resnet block sums up the inputs it sees with its outputs.

00:02:40.194 --> 00:02:43.310
So, here, the residual function is called F of x

00:02:43.310 --> 00:02:46.805
and it's just a couple of what are usually convolutional layers.

00:02:46.805 --> 00:02:50.165
After an input x travels through these two convolutional layers,

00:02:50.164 --> 00:02:53.034
F of x, then we add these two things up.

00:02:53.034 --> 00:02:56.000
So, Resnet block creates a connection between the output of

00:02:56.000 --> 00:02:59.240
these layers with the original input using a simple summation.

00:02:59.240 --> 00:03:04.250
So, instead of one layer that warrants a direct mapping between input x and output Y,

00:03:04.250 --> 00:03:07.340
a Resnet block learns a different kind of mapping from

00:03:07.340 --> 00:03:11.064
an input x to x plus this residual function.

00:03:11.064 --> 00:03:14.810
I've written a bit more about the mathematical details here, but basically,

00:03:14.810 --> 00:03:16.689
with the addition of a residual block,

00:03:16.689 --> 00:03:22.180
we've gone from a direct mapping to one that relies on a residual function F of x.

00:03:22.180 --> 00:03:27.020
It turns out to be easier to optimize this residual function than the direct mapping.

00:03:27.020 --> 00:03:30.140
All right. So, that's a lot of information about residual blocks.

00:03:30.139 --> 00:03:33.034
The way we'll build them will actually be quite simple.

00:03:33.034 --> 00:03:37.460
The residual block class is just an extension of the module class.

00:03:37.460 --> 00:03:38.689
It's defined just like

00:03:38.689 --> 00:03:42.530
any other neural network with an init function and a void function.

00:03:42.530 --> 00:03:44.585
Our residual blocks will consist of

00:03:44.585 --> 00:03:48.860
two convolutional layers with batch normalization applied to their outputs.

00:03:48.860 --> 00:03:51.770
Together, these layers should take in some input and

00:03:51.770 --> 00:03:54.680
produce an output of the exact same size.

00:03:54.680 --> 00:03:58.030
I suggest you use a Kernel size of three for the convolutional layers.

00:03:58.030 --> 00:04:02.919
You should also apply a ReLu activation to the outputs of the first convolutional layer.

00:04:02.919 --> 00:04:07.479
You can use our helper conv function from above to help you complete this class.

00:04:07.479 --> 00:04:09.829
Completing this residual block will start you

00:04:09.830 --> 00:04:12.335
on your way to completing the generator class.

00:04:12.335 --> 00:04:15.730
I'll talk about the entire generator architecture next.

