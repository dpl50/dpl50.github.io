WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.080
To make sure that our target image has the same content as our content image,

00:00:04.080 --> 00:00:06.570
we formalize the idea of a content loss,

00:00:06.570 --> 00:00:09.585
that compares the content representations of the two images.

00:00:09.585 --> 00:00:11.730
Next, we want to do the same thing for

00:00:11.730 --> 00:00:15.525
the style representations of our target image and style image.

00:00:15.525 --> 00:00:19.484
The style representation of an image relies on looking at correlations

00:00:19.484 --> 00:00:23.480
between the features in individual layers of the VGG-19 network,

00:00:23.480 --> 00:00:27.350
in other words looking at how similar the features in a single layer are.

00:00:27.350 --> 00:00:31.450
Similarities will include the general colors and textures found in that layer.

00:00:31.449 --> 00:00:36.219
We typically find the similarities between features in multiple layers in the network.

00:00:36.219 --> 00:00:40.159
By including the correlations between multiple layers of different sizes,

00:00:40.159 --> 00:00:44.479
we can obtain a multiscale style representation of the input image,

00:00:44.479 --> 00:00:47.454
one that captures large and small style features.

00:00:47.454 --> 00:00:51.140
The style representation is calculated as an image passes through

00:00:51.140 --> 00:00:55.250
the network at the first convolutional layer in all five stacks,

00:00:55.250 --> 00:00:58.994
conv1_1, conv2_1, up to conv5_1.

00:00:58.994 --> 00:01:02.759
The correlations at each layer are given by a Gram matrix.

00:01:02.759 --> 00:01:05.780
The matrix is a result of a couple of operations,

00:01:05.780 --> 00:01:08.219
and it's easiest to see in a simple example.

00:01:08.219 --> 00:01:11.094
Say, we start off with a four by four image,

00:01:11.094 --> 00:01:15.405
and we convolve it with eight different image filters to create a convolutional layer.

00:01:15.405 --> 00:01:17.960
This layer will be four by four in height and width,

00:01:17.959 --> 00:01:19.265
and eight in depth.

00:01:19.265 --> 00:01:22.189
Thinking about the style representation for this layer,

00:01:22.189 --> 00:01:23.480
we can say that this layer has

00:01:23.480 --> 00:01:26.814
eight feature maps that we want to find the relationships between.

00:01:26.814 --> 00:01:29.349
The first step in calculating the Gram matrix,

00:01:29.349 --> 00:01:31.649
will be to vectorize the values in this layer.

00:01:31.650 --> 00:01:33.890
This is very similar to what you've seen before,

00:01:33.890 --> 00:01:37.939
in the case of vectorizing an image so that it can be seen by an NLP.

00:01:37.939 --> 00:01:40.759
The first row of four values in the feature map,

00:01:40.760 --> 00:01:44.185
will become the first four values in a vector with length 16.

00:01:44.185 --> 00:01:47.635
The last row will be the last four values in that vector.

00:01:47.635 --> 00:01:50.570
By flattening the XY dimensions of the feature maps,

00:01:50.569 --> 00:01:55.444
we're converting a 3D convolutional layer into a 2D matrix of values.

00:01:55.444 --> 00:01:59.349
The next step is to multiply this matrix by its transpose.

00:01:59.349 --> 00:02:04.030
Essentially, multiplying the features in each map to get the gram matrix.

00:02:04.030 --> 00:02:08.840
This operation treats each value in the feature map as an individual sample,

00:02:08.840 --> 00:02:11.265
unrelated in space to other values.

00:02:11.264 --> 00:02:15.919
So, the resultant Gram matrix contains non-localized information about the layer.

00:02:15.919 --> 00:02:18.649
Non-localized information, is information that would

00:02:18.650 --> 00:02:21.784
still be there even if an image was shuffled around in space.

00:02:21.784 --> 00:02:25.818
For example, even if the content of a filtered image is not identifiable,

00:02:25.818 --> 00:02:29.589
you should still be able to see prominent colors and textures the style.

00:02:29.590 --> 00:02:33.200
Finally, we're left with the square eight by eight Gram matrix,

00:02:33.199 --> 00:02:36.219
whose values indicate the similarities between the the layers.

00:02:36.219 --> 00:02:38.569
So, G row four column two,

00:02:38.569 --> 00:02:40.879
will hold a value that indicates the similarity

00:02:40.879 --> 00:02:43.939
between the fourth and second feature maps in a layer.

00:02:43.939 --> 00:02:47.180
Importantly, the dimensions of this matrix are related

00:02:47.180 --> 00:02:50.659
only to the number of feature maps in the convolutional layer,

00:02:50.659 --> 00:02:53.615
it doesn't depend on the dimensions of the input image.

00:02:53.615 --> 00:02:55.550
I should note that the Gram matrix is

00:02:55.550 --> 00:03:00.515
just one mathematical way of representing the idea of shared in prominent styles.

00:03:00.514 --> 00:03:04.454
Style itself is an abstract idea but the Gram matrix,

00:03:04.455 --> 00:03:06.780
is the most widely used in practice.

00:03:06.780 --> 00:03:09.080
Now that we've defined the Gram matrix as having

00:03:09.080 --> 00:03:11.310
information about the style of a given layer,

00:03:11.310 --> 00:03:13.759
next we can calculate a style loss that compares

00:03:13.759 --> 00:03:16.799
the style of our target image and our style image.

