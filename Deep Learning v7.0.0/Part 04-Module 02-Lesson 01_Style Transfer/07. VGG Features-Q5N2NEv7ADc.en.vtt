WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.120
In this notebook, I'm going to go over an implementation of style transfer,

00:00:04.120 --> 00:00:06.589
following the details outlined in this paper.

00:00:06.589 --> 00:00:10.025
Image style transfer using convolutional neural networks.

00:00:10.025 --> 00:00:14.519
We're going to use a pre-trained VGG 19 net as a feature extractor.

00:00:14.519 --> 00:00:17.550
We can put individual images through this network,

00:00:17.550 --> 00:00:19.769
then at specific layers get the output,

00:00:19.769 --> 00:00:23.535
and calculate the content and style representations for an image.

00:00:23.535 --> 00:00:27.870
Basically, style transfer aims to create a new target image that tries to match

00:00:27.870 --> 00:00:32.314
the content of a given content image and the style of a given style image.

00:00:32.314 --> 00:00:36.489
Here's our example of a cat and a Hokusai wave image as an example.

00:00:36.490 --> 00:00:38.200
But with the code in this notebook,

00:00:38.200 --> 00:00:39.630
which is in our public GitHub,

00:00:39.630 --> 00:00:44.165
you'll be able to upload images of your own and really customize your own target image.

00:00:44.164 --> 00:00:45.829
Okay. So, first things first,

00:00:45.829 --> 00:00:49.100
I'm loading in our usual libraries including a new one,

00:00:49.100 --> 00:00:51.005
the PIL image library.

00:00:51.005 --> 00:00:54.310
This will help me load in any kind of image I want to.

00:00:54.310 --> 00:00:55.590
Next, I want to load in

00:00:55.590 --> 00:00:59.609
the pre-trained VGG 19 network that this implementation relies on.

00:00:59.609 --> 00:01:01.335
Using pi torches models,

00:01:01.335 --> 00:01:04.864
I can load this network in by name and ask for it to be pretrained.

00:01:04.864 --> 00:01:08.989
I actually just want to load in all the convolutional and pooling layers,

00:01:08.989 --> 00:01:10.969
which in this case are named features,

00:01:10.969 --> 00:01:13.075
and this is unique to the VGG network.

00:01:13.075 --> 00:01:16.255
You may remember doing something similar in the transfer learning lesson.

00:01:16.254 --> 00:01:21.409
We load in a model and we freeze any weights or parameters that we don't want to change.

00:01:21.409 --> 00:01:23.459
So, I'm saving this pre-trained model,

00:01:23.459 --> 00:01:25.439
then for every weight in this network,

00:01:25.439 --> 00:01:27.784
I'm setting requires grad to false.

00:01:27.784 --> 00:01:30.244
This means that none of these weights will change.

00:01:30.245 --> 00:01:34.130
So now, VGG becomes a kind of fixed feature extractor,

00:01:34.129 --> 00:01:37.899
which is just what we want for getting content and style features later.

00:01:37.900 --> 00:01:41.344
Next, I'm going to check if a GPU device is available,

00:01:41.344 --> 00:01:44.260
and if it is, I'm moving my model to it.

00:01:44.260 --> 00:01:46.660
I do recommend running this example on a GPU

00:01:46.659 --> 00:01:49.629
just to speed up the target image creation process.

00:01:49.629 --> 00:01:53.579
Then, this is going to print out the VGG model and all its layers.

00:01:53.579 --> 00:01:56.375
We can see the sequence of layers is all numbered.

00:01:56.375 --> 00:01:58.314
Here's the first convolutional layer,

00:01:58.314 --> 00:01:59.459
and the first stack,

00:01:59.459 --> 00:02:01.834
conv11, and its number zero.

00:02:01.834 --> 00:02:03.794
You have the second in that first stack,

00:02:03.795 --> 00:02:05.924
conv12, and it's labeled two.

00:02:05.924 --> 00:02:10.074
Then, we have a max pooling layer and conv21 in our second stack.

00:02:10.074 --> 00:02:13.869
We can keep going until the very last max pooling layer.

00:02:13.870 --> 00:02:18.110
Next, I'm going to continue loading and the resources I need to implement style transfer.

00:02:18.110 --> 00:02:20.600
So, I have my trained VGG model,

00:02:20.599 --> 00:02:23.694
and now I need to load in my content and style images.

00:02:23.694 --> 00:02:24.979
Here I have a function,

00:02:24.979 --> 00:02:28.764
which is going to transform any image into a normalized tensor.

00:02:28.764 --> 00:02:31.294
This will deal with jpegs or PNGs,

00:02:31.294 --> 00:02:35.059
and it will make sure that the size is reasonable for our purposes.

00:02:35.060 --> 00:02:40.175
Then, I'm going to actually load in style and content images from my images directory.

00:02:40.175 --> 00:02:44.765
I'm also reshaping my style image into the same shape as the content image.

00:02:44.764 --> 00:02:48.909
This reshaping step is just going to make the math nicely lined up later on.

00:02:48.909 --> 00:02:51.574
Then here, I also have a function to help me convert

00:02:51.574 --> 00:02:55.699
a normalized tensor image back into a numpy image for display,

00:02:55.699 --> 00:02:57.829
and I can show you the images that I chose.

00:02:57.830 --> 00:03:03.110
I chose an octopus for my content image and a David Hockney painting for my style image.

00:03:03.110 --> 00:03:05.180
I really like to do people or animals for

00:03:05.180 --> 00:03:08.650
content images and bright artistic paintings for style images.

00:03:08.650 --> 00:03:11.159
But it's really going to be up to you in this case.

00:03:11.159 --> 00:03:14.909
Okay. So now, we have all the elements we need for style transfer.

00:03:14.909 --> 00:03:17.620
Next, I'm going to give you your first task.

00:03:17.620 --> 00:03:19.775
We know that we have to eventually pass

00:03:19.775 --> 00:03:23.069
our content and style images through our VGG network,

00:03:23.069 --> 00:03:26.620
and extract content and style features from particular layers.

00:03:26.620 --> 00:03:30.590
So, your job is going to be to complete this get features function.

00:03:30.590 --> 00:03:34.219
This function takes in an image and returns the outputs

00:03:34.219 --> 00:03:38.060
from layers that correspond to our content and style representations.

00:03:38.060 --> 00:03:40.939
This is going to be a list of features that are taken at

00:03:40.939 --> 00:03:43.854
particular layers in our VGG 19 model.

00:03:43.854 --> 00:03:45.905
This function is almost complete.

00:03:45.905 --> 00:03:50.115
It just needs a descriptive dictionary that maps our VGG 19 layers,

00:03:50.115 --> 00:03:52.695
that are currently numbered 0 through 36,

00:03:52.694 --> 00:03:54.484
into names like conv1_1,

00:03:54.485 --> 00:03:56.290
conv2_1, and so on.

00:03:56.289 --> 00:03:59.439
I've given you the first layer that we're interested in to start.

00:03:59.439 --> 00:04:01.835
If you need a reminder for which layers make up

00:04:01.835 --> 00:04:04.485
the content and style representations of an image,

00:04:04.485 --> 00:04:06.440
take a look at the original paper,

00:04:06.439 --> 00:04:08.474
and identify which layers we'll need,

00:04:08.474 --> 00:04:10.710
then list them all here.

00:04:10.710 --> 00:04:13.120
If you get stuck or don't know where to start,

00:04:13.120 --> 00:04:15.469
I'll show you my solution in the next video.

