WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.359
When a CNN is trained to classify images,

00:00:03.359 --> 00:00:05.445
it's convolutional layers learn to extract

00:00:05.445 --> 00:00:08.349
more and more complex features from a given image.

00:00:08.349 --> 00:00:12.899
Intermittently, max pooling layers will discard detailed spatial information,

00:00:12.900 --> 00:00:16.879
information that's increasingly irrelevant to the task of classification.

00:00:16.879 --> 00:00:20.640
The effect of this is that as we go deeper into a CNN,

00:00:20.640 --> 00:00:24.660
the input image is transformed into feature maps that increasingly care about

00:00:24.660 --> 00:00:29.565
the content of the image rather than any detail about the texture and color of pixels.

00:00:29.565 --> 00:00:32.280
Later layers of a network are even sometimes referred

00:00:32.280 --> 00:00:35.435
to as a content representation of an image.

00:00:35.435 --> 00:00:38.039
In this way, a trained CNN has already learned to

00:00:38.039 --> 00:00:41.155
represent the content of an image, but what about style?

00:00:41.155 --> 00:00:45.515
Style can be thought of as traits that might be found in the brush strokes of a painting,

00:00:45.515 --> 00:00:48.304
its texture, colors, curvature, and so on.

00:00:48.304 --> 00:00:49.744
To perform style transfer,

00:00:49.744 --> 00:00:53.500
we need to combine the content of one image with the style of another.

00:00:53.500 --> 00:00:56.505
So, how can we isolate only the style of an image?

00:00:56.505 --> 00:00:59.030
To represent the style of an input image,

00:00:59.030 --> 00:01:03.105
a feature space designed to capture texture and color information is used.

00:01:03.104 --> 00:01:07.420
This space essentially looks at spatial correlations within a layer of a network.

00:01:07.420 --> 00:01:12.409
A correlation is a measure of the relationship between two or more variables.

00:01:12.409 --> 00:01:15.229
For example, you could look at the features extracted

00:01:15.230 --> 00:01:18.305
in the first convolutional layer which has some depth.

00:01:18.305 --> 00:01:21.825
The depth corresponds to the number of feature maps in that layer.

00:01:21.825 --> 00:01:23.255
For each feature map,

00:01:23.254 --> 00:01:24.560
we can measure how strongly

00:01:24.560 --> 00:01:28.564
its detected features relate to the other feature maps in that layer.

00:01:28.564 --> 00:01:32.884
Is a certain color detected in one map similar to a color in another map?

00:01:32.885 --> 00:01:36.704
What about the differences between detected edges and corners, and so on?

00:01:36.704 --> 00:01:41.564
See which colors and shapes in a set of feature maps are related and which are not.

00:01:41.564 --> 00:01:44.120
Say, we detect that mini-feature maps in

00:01:44.120 --> 00:01:47.775
the first convolutional layer have similar pink edge features.

00:01:47.775 --> 00:01:50.900
If there are common colors and shapes among the feature maps,

00:01:50.900 --> 00:01:53.680
then this can be thought of as part of that image's style.

00:01:53.680 --> 00:01:57.890
So, the similarities and differences between features in a layer should

00:01:57.890 --> 00:02:02.495
give us some information about the texture and color information found in an image.

00:02:02.495 --> 00:02:03.890
But at the same time,

00:02:03.890 --> 00:02:06.049
it should leave out information about

00:02:06.049 --> 00:02:09.969
the actual arrangement and identity of different objects in that image.

00:02:09.969 --> 00:02:14.155
Now, we've seen that content and style can be separate components of an image.

00:02:14.155 --> 00:02:17.310
Let's think about this in a complete style transfer example.

00:02:17.310 --> 00:02:20.384
Style transfer will look at two different images.

00:02:20.384 --> 00:02:23.780
We often call these the style image and the content image.

00:02:23.780 --> 00:02:25.344
Using a trained CNN,

00:02:25.344 --> 00:02:29.425
style transfer finds the style of one image and the content of the other.

00:02:29.425 --> 00:02:33.120
Finally, it tries to merge the two to create a new third image.

00:02:33.120 --> 00:02:34.740
In this newly created image,

00:02:34.740 --> 00:02:38.240
the objects and their arrangement are taken from the content image,

00:02:38.240 --> 00:02:41.135
and the colors and textures are taken from the style image.

00:02:41.134 --> 00:02:43.389
Here's our example of an image of a cat,

00:02:43.389 --> 00:02:48.039
the content image, being combined with a Hokusai-style image of waves.

00:02:48.039 --> 00:02:52.314
Effectively, style transfer creates a new image that keeps the cat content,

00:02:52.314 --> 00:02:53.759
but renders it with the colors,

00:02:53.759 --> 00:02:57.034
the print texture, the style of the wave artwork.

00:02:57.034 --> 00:03:00.104
This is the theory behind how style transfer works.

00:03:00.104 --> 00:03:04.669
Next, let's talk more about how we can actually extract features from different layers of

00:03:04.669 --> 00:03:09.919
a trained model and use them to combine the style and content of two different images.

