<!-- udacimak v1.4.4 -->
<!DOCTYPE html>
<html lang="en">
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <meta content="ie=edge" http-equiv="X-UA-Compatible"/>
  <title>
   What is Batch Normalization?
  </title>
  <link href="../assets/css/bootstrap.min.css" rel="stylesheet"/>
  <link href="../assets/css/plyr.css" rel="stylesheet"/>
  <link href="../assets/css/katex.min.css" rel="stylesheet"/>
  <link href="../assets/css/jquery.mCustomScrollbar.min.css" rel="stylesheet"/>
  <link href="../assets/css/styles.css" rel="stylesheet"/>
  <link href="../assets/img/udacimak.png" rel="shortcut icon" type="image/png">
  </link>
 </head>
 <body>
  <div class="wrapper">
   <nav id="sidebar">
    <div class="sidebar-header">
     <h3>
      Deep Convolutional GANs
     </h3>
    </div>
    <ul class="sidebar-list list-unstyled CTAs">
     <li>
      <a class="article" href="../index.html">
       Back to Home
      </a>
     </li>
    </ul>
    <ul class="sidebar-list list-unstyled components">
     <li class="">
      <a href="01. Deep Convolutional GANs.html">
       01. Deep Convolutional GANs
      </a>
     </li>
     <li class="">
      <a href="02. DCGAN, Discriminator.html">
       02. DCGAN, Discriminator
      </a>
     </li>
     <li class="">
      <a href="03. DCGAN Generator.html">
       03. DCGAN Generator
      </a>
     </li>
     <li class="">
      <a href="04. What is Batch Normalization.html">
       04. What is Batch Normalization?
      </a>
     </li>
     <li class="">
      <a href="05. Pre-Notebook Batch Norm.html">
       05. Pre-Notebook: Batch Norm
      </a>
     </li>
     <li class="">
      <a href="06. Notebook Batch Norm.html">
       06. Notebook: Batch Norm
      </a>
     </li>
     <li class="">
      <a href="07. Benefits of Batch Normalization.html">
       07. Benefits of Batch Normalization
      </a>
     </li>
     <li class="">
      <a href="08. DCGAN Notebook &amp; Data.html">
       08. DCGAN Notebook &amp; Data
      </a>
     </li>
     <li class="">
      <a href="09. Pre-Notebook DCGAN, SVHN.html">
       09. Pre-Notebook: DCGAN, SVHN
      </a>
     </li>
     <li class="">
      <a href="10. Notebook DCGAN, SVHN.html">
       10. Notebook: DCGAN, SVHN
      </a>
     </li>
     <li class="">
      <a href="11. Scaling, Solution.html">
       11. Scaling, Solution
      </a>
     </li>
     <li class="">
      <a href="12. Discriminator.html">
       12. Discriminator
      </a>
     </li>
     <li class="">
      <a href="13. Discriminator, Solution.html">
       13. Discriminator, Solution
      </a>
     </li>
     <li class="">
      <a href="14. Generator.html">
       14. Generator
      </a>
     </li>
     <li class="">
      <a href="15. Generator, Solution.html">
       15. Generator, Solution
      </a>
     </li>
     <li class="">
      <a href="16. Optimization Strategy.html">
       16. Optimization Strategy
      </a>
     </li>
     <li class="">
      <a href="17. Optimization Solution &amp; Samples.html">
       17. Optimization Solution &amp; Samples
      </a>
     </li>
     <li class="">
      <a href="18. Other Applications of GANs.html">
       18. Other Applications of GANs
      </a>
     </li>
    </ul>
    <ul class="sidebar-list list-unstyled CTAs">
     <li>
      <a class="article" href="../index.html">
       Back to Home
      </a>
     </li>
    </ul>
   </nav>
   <div id="content">
    <header class="container-fluild header">
     <div class="container">
      <div class="row">
       <div class="col-12">
        <div class="align-items-middle">
         <button class="btn btn-toggle-sidebar" id="sidebarCollapse" type="button">
          <div>
          </div>
          <div>
          </div>
          <div>
          </div>
         </button>
         <h1 style="display: inline-block">
          04. What is Batch Normalization?
         </h1>
        </div>
       </div>
      </div>
     </div>
    </header>
    <main class="container">
     <div class="row">
      <div class="col-12">
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <h1 id="what-is-batch-normalization">
          What is Batch Normalization?
         </h1>
         <p>
          Batch normalization was introduced in Sergey Ioffe's and Christian Szegedy's 2015 paper
          <a href="https://arxiv.org/pdf/1502.03167.pdf" rel="noopener noreferrer" target="_blank">
           Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift
          </a>
          . The idea is that, instead of just normalizing the inputs to the network, we normalize the inputs to every layer
          <em>
           within
          </em>
          the network.
         </p>
         <h3 id="batch-normalization">
          Batch Normalization
         </h3>
         <p>
          It's called "batch" normalization because, during
          <strong>
           training
          </strong>
          , we normalize each layer's inputs by using the
          <em>
           mean
          </em>
          and
          <em>
           standard deviation
          </em>
          (or variance) of the values in the current batch. These are sometimes called the
          <strong>
           batch statistics
          </strong>
          .
         </p>
         <blockquote>
          <p>
           Specifically, batch normalization normalizes the output of a previous layer by
           <strong>
            subtracting the batch mean and dividing by the batch standard deviation
           </strong>
           .
          </p>
         </blockquote>
         <p>
          Why might this help? Well, we know that normalizing the inputs to a network helps the network learn and converge to a solution. However, a network is a series of layers, where the output of one layer becomes the input to another. That means we can think of any layer in a neural network as the
          <em>
           first
          </em>
          layer of a smaller network.
         </p>
         <h3 id="normalization-at-every-layer">
          Normalization at Every Layer
         </h3>
         <p>
          For example, imagine a 3 layer network.
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <figure class="figure">
          <img alt="3 layer network" class="img img-fluid" src="img/3-layers.png"/>
          <figcaption class="figure-caption">
           <p>
            3 layer network
           </p>
          </figcaption>
         </figure>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          Instead of just thinking of it as a single network with inputs, layers, and outputs, think of the output of layer 1 as the input to a two layer network. This two layer network would consist of layers 2 and 3 in our original network.
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <figure class="figure">
          <img alt="2 layer network" class="img img-fluid" src="img/2-layers.png"/>
          <figcaption class="figure-caption">
           <p>
            2 layer network
           </p>
          </figcaption>
         </figure>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          Likewise, the output of layer 2 can be thought of as the input to a single layer network, consisting only of layer 3.
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <figure class="figure">
          <img alt="One layer network" class="img img-fluid" src="img/one-layer.png"/>
          <figcaption class="figure-caption">
           <p>
            One layer network
           </p>
          </figcaption>
         </figure>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <p>
          When you think of it like this - as a series of neural networks feeding into each other - then it's easy to imagine how normalizing the inputs to each layer would help. It's just like normalizing the inputs to any other neural network, but you're doing it at
          <strong>
           every layer (sub-network)
          </strong>
          .
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <h2 id="internal-covariate-shift">
          Internal Covariate Shift
         </h2>
         <p>
          Beyond the intuitive reasons, there are good mathematical reasons to motivate batch normalization. It helps combat what the authors call
          <strong>
           internal covariate shift
          </strong>
          .
         </p>
         <blockquote>
          <p>
           In this case, internal covariate shift refers to the change in the distribution of the inputs to different layers. It turns out that training a network is most efficient when the distribution of inputs to each layer is similar!
          </p>
         </blockquote>
         <p>
          And batch normalization is one method of standardizing the distribution of layer inputs. This discussion is best handled
          <a href="https://arxiv.org/pdf/1502.03167.pdf" rel="noopener noreferrer" target="_blank">
           in the paper
          </a>
          and in
          <a href="http://www.deeplearningbook.org" rel="noopener noreferrer" target="_blank">
           Deep Learning
          </a>
          , a book you can read online written by Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Specifically, check out the batch normalization section of
          <a href="http://www.deeplearningbook.org/contents/optimization.html" rel="noopener noreferrer" target="_blank">
           Chapter 8: Optimization for Training Deep Models
          </a>
          .
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <hr/>
         <h1 id="the-math">
          The Math
         </h1>
         <p>
          Next, let's do a deep dive into the math behind batch normalization. This is not critical for you to know, but it may help your understanding of this whole process!
         </p>
         <h2 id="getting-the-mean-and-variance">
          Getting the mean and variance
         </h2>
         <p>
          In order to normalize the values, we first need to find the average value for the batch. If you look at the code, you can see that this is not the average value of the batch
          <em>
           inputs
          </em>
          , but the average value coming
          <em>
           out
          </em>
          of any particular layer before we pass it through its non-linear activation function and then feed it as an input to the
          <em>
           next
          </em>
          layer.
         </p>
         <p>
          We represent the average as
          <strong>
           mu_B
          </strong>
          <div class="mathquill">
           \mu_B
          </div>
          which is simply the sum of all of the values,
          <strong>
           x_i
          </strong>
          divided by the number of values,
          <strong>
           m
          </strong>
          .
         </p>
         <div class="mathquill">
          \mu_B \leftarrow \frac{1}{m}\sum_{i=1}^m x_i
         </div>
         <p>
          We then need to calculate the variance, or mean squared deviation, represented as
          <div class="mathquill">
           \sigma_{B}^{2}
          </div>
         </p>
         <p>
          If you aren't familiar with statistics, that simply means for each value
          <strong>
           x_i
          </strong>
          , we subtract the average value (calculated earlier as
          <strong>
           mu_B
          </strong>
          ), which gives us what's called the "deviation" for that value. We square the result to get the squared deviation. Sum up the results of doing that for each of the values, then divide by the number of values, again
          <strong>
           m
          </strong>
          , to get the average, or mean, squared deviation.
         </p>
         <div class="mathquill">
          \sigma_{B}^{2} \leftarrow \frac{1}{m}\sum_{i=1}^m (x_i - \mu_B)^2
         </div>
         <h2 id="normalizing-output-values">
          Normalizing output values
         </h2>
         <p>
          Once we have the mean and variance, we can use them to normalize the values with the following equation. For each value, it subtracts the mean and divides by the (almost) standard deviation. (You've probably heard of standard deviation many times, but if you have not studied statistics you might not know that the standard deviation is actually the square root of the mean squared deviation.)
         </p>
         <div class="mathquill">
          \hat{x_i} \leftarrow \frac{x_i - \mu_B}{\sqrt{\sigma_{B}^{2} + \epsilon}}
         </div>
         <p>
          Above, we said "(almost) standard deviation". That's because the real standard deviation for the batch is calculated by
          <div class="mathquill">
           \sqrt{\sigma_{B}^{2}}
          </div>
         </p>
         <p>
          but the above formula adds the term epsilon before taking the square root. The epsilon can be any small, positive constant, ex. the value
          <code>
           0.001
          </code>
          . It is there partially to make sure we don't try to divide by zero, but it also acts to increase the variance slightly for each batch.
         </p>
         <p>
          Why add this extra value and mimic an increase in variance? Statistically, this makes sense because even though we are normalizing one batch at a time, we are also trying to estimate the population distribution – the total training set, which itself an estimate of the larger population of inputs your network wants to handle. The variance of a population is
          <em>
           typically
          </em>
          higher than the variance for any sample taken from that population, especially when you use a small sample size (a small sample is more likely to include values near the peak of a population distribution), so increasing the variance a little bit for each batch helps take that into account.
         </p>
         <p>
          At this point, we have a normalized value, represented as
          <div class="mathquill">
           \hat{x_i}
          </div>
         </p>
         <p>
          But rather than use it directly, we multiply it by a
          <strong>
           gamma
          </strong>
          value, and then add a
          <strong>
           beta
          </strong>
          value. Both gamma and beta are learnable parameters of the network and serve to scale and shift the normalized value, respectively. Because they are learnable just like weights, they give your network some extra knobs to tweak during training to help it learn the function it is trying to approximate.
         </p>
         <div class="mathquill">
          y_i \leftarrow \gamma \hat{x_i} + \beta
         </div>
         <p>
          We now have the final batch-normalized output of our layer, which we would then pass to a non-linear activation function like sigmoid, tanh, ReLU, Leaky ReLU, etc. In the original batch normalization paper, they mention that there might be cases when you'd want to perform the batch normalization
          <em>
           after
          </em>
          the non-linearity instead of before, but it is difficult to find any uses like that in practice.
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <h4 id="next-take-a-look-at-the-effect-of-batch-normalization-by-applying-it-to-a-pytorch-model">
          Next, take a look at the effect of batch normalization, by applying it to a PyTorch model!
         </h4>
        </div>
       </div>
       <div class="divider">
       </div>
      </div>
      <div class="col-12">
       <p class="text-right">
        <a class="btn btn-outline-primary mt-4" href="05. Pre-Notebook Batch Norm.html" role="button">
         Next Concept
        </a>
       </p>
      </div>
     </div>
    </main>
    <footer class="footer">
     <div class="container">
      <div class="row">
       <div class="col-12">
        <p class="text-center">
         udacity2.0 If you need the newest courses Plase add me wechat: udacity6
        </p>
       </div>
      </div>
     </div>
    </footer>
   </div>
  </div>
  <script src="../assets/js/jquery-3.3.1.min.js">
  </script>
  <script src="../assets/js/plyr.polyfilled.min.js">
  </script>
  <script src="../assets/js/bootstrap.min.js">
  </script>
  <script src="../assets/js/jquery.mCustomScrollbar.concat.min.js">
  </script>
  <script src="../assets/js/katex.min.js">
  </script>
  <script>
   // Initialize Plyr video players
    const players = Array.from(document.querySelectorAll('video')).map(p => new Plyr(p));

    // render math equations
    let elMath = document.getElementsByClassName('mathquill');
    for (let i = 0, len = elMath.length; i < len; i += 1) {
      const el = elMath[i];

      katex.render(el.textContent, el, {
        throwOnError: false
      });
    }

    // this hack will make sure Bootstrap tabs work when using Handlebars
    if ($('#question-tabs').length && $('#user-answer-tabs').length) {
      $("#question-tabs a.nav-link").on('click', function () {
        $("#question-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
      $("#user-answer-tabs a.nav-link").on('click', function () {
        $("#user-answer-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
    } else {
      $("a.nav-link").on('click', function () {
        $(".tab-pane").hide();
        $($(this).attr("href")).show();
      });
    }

    // side bar events
    $(document).ready(function () {
      $("#sidebar").mCustomScrollbar({
        theme: "minimal"
      });

      $('#sidebarCollapse').on('click', function () {
        $('#sidebar, #content').toggleClass('active');
        $('.collapse.in').toggleClass('in');
        $('a[aria-expanded=true]').attr('aria-expanded', 'false');
      });

      // scroll to first video on page loading
      if ($('video').length) {
        $('html,body').animate({ scrollTop: $('div.plyr').prev().offset().top});
      }

      // auto play first video: this may not work with chrome/safari due to autoplay policy
      if (players && players.length > 0) {
        players[0].play();
      }

      // scroll sidebar to current concept
      const currentInSideBar = $( "ul.sidebar-list.components li a:contains('04. What is Batch Normalization?')" )
      currentInSideBar.css( "text-decoration", "underline" );
      $("#sidebar").mCustomScrollbar('scrollTo', currentInSideBar);
    });
  </script>
 </body>
</html>
