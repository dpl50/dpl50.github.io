<!-- udacimak v1.4.4 -->
<!DOCTYPE html>
<html lang="en">
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1.0" name="viewport"/>
  <meta content="ie=edge" http-equiv="X-UA-Compatible"/>
  <title>
   Better Weight Initialization Strategy
  </title>
  <link href="../assets/css/bootstrap.min.css" rel="stylesheet"/>
  <link href="../assets/css/plyr.css" rel="stylesheet"/>
  <link href="../assets/css/katex.min.css" rel="stylesheet"/>
  <link href="../assets/css/jquery.mCustomScrollbar.min.css" rel="stylesheet"/>
  <link href="../assets/css/styles.css" rel="stylesheet"/>
  <link href="../assets/img/udacimak.png" rel="shortcut icon" type="image/png">
  </link>
 </head>
 <body>
  <div class="wrapper">
   <nav id="sidebar">
    <div class="sidebar-header">
     <h3>
      Sentiment Analysis
     </h3>
    </div>
    <ul class="sidebar-list list-unstyled CTAs">
     <li>
      <a class="article" href="../index.html">
       Back to Home
      </a>
     </li>
    </ul>
    <ul class="sidebar-list list-unstyled components">
     <li class="">
      <a href="01. Introducing Andrew Trask.html">
       01. Introducing Andrew Trask
      </a>
     </li>
     <li class="">
      <a href="02. Meet Andrew.html">
       02. Meet Andrew
      </a>
     </li>
     <li class="">
      <a href="03. Materials.html">
       03. Materials
      </a>
     </li>
     <li class="">
      <a href="04. The Notebooks.html">
       04. The Notebooks
      </a>
     </li>
     <li class="">
      <a href="05. Framing the Problem.html">
       05. Framing the Problem
      </a>
     </li>
     <li class="">
      <a href="06. Mini Project 1.html">
       06. Mini Project 1
      </a>
     </li>
     <li class="">
      <a href="07. Mini Project 1 Solution.html">
       07. Mini Project 1 Solution
      </a>
     </li>
     <li class="">
      <a href="08. Transforming Text into Numbers.html">
       08. Transforming Text into Numbers
      </a>
     </li>
     <li class="">
      <a href="09. Mini Project 2.html">
       09. Mini Project 2
      </a>
     </li>
     <li class="">
      <a href="10. Mini Project 2 Solution.html">
       10. Mini Project 2 Solution
      </a>
     </li>
     <li class="">
      <a href="11. Building a Neural Network.html">
       11. Building a Neural Network
      </a>
     </li>
     <li class="">
      <a href="12. Mini Project 3.html">
       12. Mini Project 3
      </a>
     </li>
     <li class="">
      <a href="13. Mini Project 3 Solution.html">
       13. Mini Project 3 Solution
      </a>
     </li>
     <li class="">
      <a href="14. Better Weight Initialization Strategy.html">
       14. Better Weight Initialization Strategy
      </a>
     </li>
     <li class="">
      <a href="15. Understanding Neural Noise.html">
       15. Understanding Neural Noise
      </a>
     </li>
     <li class="">
      <a href="16. Mini Project 4.html">
       16. Mini Project 4
      </a>
     </li>
     <li class="">
      <a href="17. Understanding Inefficiencies in our Network.html">
       17. Understanding Inefficiencies in our Network
      </a>
     </li>
     <li class="">
      <a href="18. Mini Project 5.html">
       18. Mini Project 5
      </a>
     </li>
     <li class="">
      <a href="19. Mini Project 5 Solution.html">
       19. Mini Project 5 Solution
      </a>
     </li>
     <li class="">
      <a href="20. Further Noise Reduction.html">
       20. Further Noise Reduction
      </a>
     </li>
     <li class="">
      <a href="21. Mini Project 6.html">
       21. Mini Project 6
      </a>
     </li>
     <li class="">
      <a href="22. Mini Project 6 Solution.html">
       22. Mini Project 6 Solution
      </a>
     </li>
     <li class="">
      <a href="23. Analysis What's Going on in the Weights.html">
       23. Analysis: What's Going on in the Weights?
      </a>
     </li>
     <li class="">
      <a href="24. Conclusion.html">
       24. Conclusion
      </a>
     </li>
    </ul>
    <ul class="sidebar-list list-unstyled CTAs">
     <li>
      <a class="article" href="../index.html">
       Back to Home
      </a>
     </li>
    </ul>
   </nav>
   <div id="content">
    <header class="container-fluild header">
     <div class="container">
      <div class="row">
       <div class="col-12">
        <div class="align-items-middle">
         <button class="btn btn-toggle-sidebar" id="sidebarCollapse" type="button">
          <div>
          </div>
          <div>
          </div>
          <div>
          </div>
         </button>
         <h1 style="display: inline-block">
          14. Better Weight Initialization Strategy
         </h1>
        </div>
       </div>
      </div>
     </div>
    </header>
    <main class="container">
     <div class="row">
      <div class="col-12">
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <h3 id="better-weight-initialization-strategy">
          Better Weight Initialization Strategy
         </h3>
         <p>
          In the last video, Andrew Trask built a neural network and initialized the weights according to the function
          <code>
           init_network
          </code>
          :
         </p>
         <pre><code>def init_network(self, input_nodes, hidden_nodes, output_nodes, learning_rate):
        # Set number of nodes in input, hidden and output layers.
        self.input_nodes = input_nodes
        self.hidden_nodes = hidden_nodes
        self.output_nodes = output_nodes

        # Store the learning rate
        self.learning_rate = learning_rate

        # Initialize weights

        # These are the weights between the input layer and the hidden layer.
        self.weights_0_1 = np.zeros((self.input_nodes,self.hidden_nodes))

        # These are the weights between the hidden layer and the output layer.
        self.weights_1_2 = np.random.normal(0.0, self.output_nodes**-0.5, 
                                                (self.hidden_nodes, self.output_nodes))

        # The input layer, a two-dimensional matrix with shape 1 x input_nodes
        self.layer_0 = np.zeros((1,input_nodes))</code></pre>
         <p>
          Here, I'd like you to take note of the line
          <code>
           self.weights_1_2 = np.random.normal(0.0, self.output_nodes**-0.5, (self.hidden_nodes, self.output_nodes))
          </code>
          this is one possible initialization strategy, but there is actually a better way to initialize these weights that I will briefly go over in this concept.
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <h3 id="different-initialization-better-accuracy">
          Different Initialization, Better Accuracy
         </h3>
         <p>
          You'll learn more about weight initialization, later on in this program. Suffice to say that the general rule for setting the weights in a neural network is to set them to be close to zero without being too small.
         </p>
         <blockquote>
          <p>
           Good practice is to start your weights in the range of
           <strong>
            [-y, y]
           </strong>
           where
          </p>
         </blockquote>
         <div class="mathquill">
          y=1/\sqrt{n}
         </div>
         <p>
          And
          <strong>
           n
          </strong>
          is the number of inputs to a given layer.
         </p>
         <p>
          In
          <em>
           this
          </em>
          case, between layers 1 and 2, a better solution would be to define the weights as a function of the number of nodes
          <strong>
           n
          </strong>
          in the
          <em>
           hidden
          </em>
          layer.
         </p>
         <p>
          So, the initialization code would change; you'd use
          <code>
           hidden_nodes**-0.5
          </code>
          rather than
          <code>
           output_nodes**-0.5
          </code>
          If you try this out, you should see that you get improved training with a larger
          <code>
           learning rate = 0.01
          </code>
          rather than waiting for a learning rate of 0.001.
         </p>
         <pre><code class="python language-python">def init_network(self, input_nodes, hidden_nodes, output_nodes, learning_rate):
        # Set number of nodes in input, hidden and output layers.
        self.input_nodes = input_nodes
        self.hidden_nodes = hidden_nodes
        self.output_nodes = output_nodes

        # Store the learning rate
        self.learning_rate = learning_rate

        # Initialize weights

        # These are the weights between the input layer and the hidden layer.
        self.weights_0_1 = np.zeros((self.input_nodes,self.hidden_nodes))

        # These are the weights between the hidden layer and the output layer.
        ## NOTE: the difference in the standard deviation of the normal weights
        ## This was changed from `self.output_nodes**-0.5` to `self.hidden_nodes**-0.5`
        self.weights_1_2 = np.random.normal(0.0, self.hidden_nodes**-0.5, 
                                                (self.hidden_nodes, self.output_nodes))

        # The input layer, a two-dimensional matrix with shape 1 x input_nodes
        self.layer_0 = np.zeros((1,input_nodes))</code></pre>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <figure class="figure">
          <img alt="Results for learning rate = 0.01." class="img img-fluid" src="img/screen-shot-2018-11-13-at-1.43.17-pm.png"/>
          <figcaption class="figure-caption">
           <p>
            Results for learning rate = 0.01.
           </p>
          </figcaption>
         </figure>
        </div>
       </div>
       <div class="divider">
       </div>
       <div class="ud-atom">
        <h3>
        </h3>
        <div>
         <h3 id="the-code">
          The Code
         </h3>
         <p>
          Andrew will still use the initialization strategy using
          <code>
           output_nodes**-0.5
          </code>
          , but you can see the solution code for this
          <em>
           alternate
          </em>
          weight initialization strategy in
          <a href="https://github.com/udacity/deep-learning-v2-pytorch/blob/master/sentiment-analysis-network/Sentiment_Classification_Solutions_2_Better_Weight_Initialization.ipynb" rel="noopener noreferrer" target="_blank">
           the Github repo
          </a>
          .
         </p>
        </div>
       </div>
       <div class="divider">
       </div>
      </div>
      <div class="col-12">
       <p class="text-right">
        <a class="btn btn-outline-primary mt-4" href="15. Understanding Neural Noise.html" role="button">
         Next Concept
        </a>
       </p>
      </div>
     </div>
    </main>
    <footer class="footer">
     <div class="container">
      <div class="row">
       <div class="col-12">
        <p class="text-center">
         udacity2.0 If you need the newest courses Plase add me wechat: udacity6
        </p>
       </div>
      </div>
     </div>
    </footer>
   </div>
  </div>
  <script src="../assets/js/jquery-3.3.1.min.js">
  </script>
  <script src="../assets/js/plyr.polyfilled.min.js">
  </script>
  <script src="../assets/js/bootstrap.min.js">
  </script>
  <script src="../assets/js/jquery.mCustomScrollbar.concat.min.js">
  </script>
  <script src="../assets/js/katex.min.js">
  </script>
  <script>
   // Initialize Plyr video players
    const players = Array.from(document.querySelectorAll('video')).map(p => new Plyr(p));

    // render math equations
    let elMath = document.getElementsByClassName('mathquill');
    for (let i = 0, len = elMath.length; i < len; i += 1) {
      const el = elMath[i];

      katex.render(el.textContent, el, {
        throwOnError: false
      });
    }

    // this hack will make sure Bootstrap tabs work when using Handlebars
    if ($('#question-tabs').length && $('#user-answer-tabs').length) {
      $("#question-tabs a.nav-link").on('click', function () {
        $("#question-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
      $("#user-answer-tabs a.nav-link").on('click', function () {
        $("#user-answer-tab-contents .tab-pane").hide();
        $($(this).attr("href")).show();
      });
    } else {
      $("a.nav-link").on('click', function () {
        $(".tab-pane").hide();
        $($(this).attr("href")).show();
      });
    }

    // side bar events
    $(document).ready(function () {
      $("#sidebar").mCustomScrollbar({
        theme: "minimal"
      });

      $('#sidebarCollapse').on('click', function () {
        $('#sidebar, #content').toggleClass('active');
        $('.collapse.in').toggleClass('in');
        $('a[aria-expanded=true]').attr('aria-expanded', 'false');
      });

      // scroll to first video on page loading
      if ($('video').length) {
        $('html,body').animate({ scrollTop: $('div.plyr').prev().offset().top});
      }

      // auto play first video: this may not work with chrome/safari due to autoplay policy
      if (players && players.length > 0) {
        players[0].play();
      }

      // scroll sidebar to current concept
      const currentInSideBar = $( "ul.sidebar-list.components li a:contains('14. Better Weight Initialization Strategy')" )
      currentInSideBar.css( "text-decoration", "underline" );
      $("#sidebar").mCustomScrollbar('scrollTo', currentInSideBar);
    });
  </script>
 </body>
</html>
