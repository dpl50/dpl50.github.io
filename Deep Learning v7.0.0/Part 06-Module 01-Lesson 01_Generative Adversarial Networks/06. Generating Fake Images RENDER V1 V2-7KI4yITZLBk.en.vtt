WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.200
Hi. So, you've just learned a lot about GANs from Ian Goodfellow.

00:00:04.200 --> 00:00:07.259
Next, I'll be going over a practical implementation.

00:00:07.259 --> 00:00:11.025
I'll be defining again and training it on our MNIST dataset.

00:00:11.025 --> 00:00:15.780
From this, you'll be able to take your train network and generate new handwritten digits.

00:00:15.779 --> 00:00:20.009
You can think of these as fakes that look like they've come from the original dataset,

00:00:20.010 --> 00:00:22.595
but are in fact generated by a neural network.

00:00:22.594 --> 00:00:26.129
There are a lot of interesting applications for which this is useful.

00:00:26.129 --> 00:00:30.375
I've seen people try to generate new musical scores based on existing data.

00:00:30.375 --> 00:00:33.299
This example is from a model called MidiNet.

00:00:33.299 --> 00:00:36.679
One of my favorite applications is a GAN that learns to generate

00:00:36.679 --> 00:00:40.234
new kinds of tulips based on a carefully collected dataset.

00:00:40.234 --> 00:00:42.725
Here's an example from a tulip generation model

00:00:42.725 --> 00:00:45.395
which was created by the artist and healer.

00:00:45.395 --> 00:00:48.800
In these cases, a trained generator has kind of learned to

00:00:48.799 --> 00:00:52.809
imagine what new pieces of data might look like, which is really cool.

00:00:52.810 --> 00:00:56.164
So, GANs were first reported on in 2014 by

00:00:56.164 --> 00:01:00.875
Ian Goodfellow who was then in Yoshua Bengio's lab at the University of Montreal.

00:01:00.875 --> 00:01:04.204
Since then, GANs had just exploded in popularity.

00:01:04.204 --> 00:01:05.974
In the next few lessons,

00:01:05.974 --> 00:01:07.929
you'll see some really great applications.

00:01:07.930 --> 00:01:13.360
Next, I'll go over our approach to building a GAN and generating new MNIST images.

