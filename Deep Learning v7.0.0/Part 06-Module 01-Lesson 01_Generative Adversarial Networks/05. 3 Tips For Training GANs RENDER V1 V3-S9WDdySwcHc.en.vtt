WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.625
To make GANs actually work really well,

00:00:02.625 --> 00:00:05.730
it's important to choose a good overall architecture.

00:00:05.730 --> 00:00:07.600
If we have a very simple task,

00:00:07.599 --> 00:00:13.184
like generating 28 by 28 pixel images of handwritten digits from the MNIST dataset,

00:00:13.185 --> 00:00:16.004
we can get away with a fully connected architecture.

00:00:16.004 --> 00:00:18.239
In a fully connected architecture,

00:00:18.239 --> 00:00:20.729
all of the interactions between layers consist of

00:00:20.730 --> 00:00:23.399
matrix multiplication by weight matrices.

00:00:23.399 --> 00:00:26.518
There's no convolution, there's no recurrence.

00:00:26.518 --> 00:00:30.959
The most important design consideration for this architecture is to make

00:00:30.960 --> 00:00:35.445
sure that both the generator and the discriminator have at least one hidden layer.

00:00:35.445 --> 00:00:39.619
That ensures that both models have a universal approximator property and

00:00:39.619 --> 00:00:43.954
can represent any probability distribution if given enough hidden units.

00:00:43.954 --> 00:00:45.559
For the hidden units,

00:00:45.560 --> 00:00:47.690
many activation functions will work,

00:00:47.689 --> 00:00:50.304
but Leaky ReLUs are especially popular.

00:00:50.304 --> 00:00:55.009
Leaky ReLUs help to make sure that the gradient can flow through the entire architecture.

00:00:55.009 --> 00:00:58.329
That's an important consideration for any machine learning model.

00:00:58.329 --> 00:01:01.549
But, it's even more important for GANs because the only way

00:01:01.549 --> 00:01:05.034
that generator can learn is to receive a gradient from the discriminator.

00:01:05.034 --> 00:01:07.039
A popular choice for the output of

00:01:07.040 --> 00:01:11.030
the generator is the hyperbolic tangent activation function.

00:01:11.030 --> 00:01:13.159
This means that your data should be scaled to

00:01:13.159 --> 00:01:16.039
the interval ranging from negative one to positive one.

00:01:16.040 --> 00:01:18.515
Finally, for most versions of GANs,

00:01:18.515 --> 00:01:21.515
the output of the discriminator needs to be a probability.

00:01:21.515 --> 00:01:23.269
To enforce that constraint,

00:01:23.269 --> 00:01:25.429
we use a sigmoid unit as the output.

00:01:25.430 --> 00:01:28.280
GANs are different from many other machine learning models because they

00:01:28.280 --> 00:01:31.870
require running two optimization algorithms simultaneously.

00:01:31.870 --> 00:01:35.359
We define a loss for the generator and a loss for the discriminator.

00:01:35.359 --> 00:01:39.319
Then, we use one optimizer to minimize the loss for the discriminator,

00:01:39.319 --> 00:01:43.939
while we simultaneously use another optimizer to minimize the loss for the generator.

00:01:43.939 --> 00:01:46.128
Adam is a good choice for the optimizer.

00:01:46.129 --> 00:01:47.900
This is one of the design choices from

00:01:47.900 --> 00:01:51.515
the DCGAN architecture developed by Indigo and Facebook.

00:01:51.515 --> 00:01:53.644
To setup the discriminator loss,

00:01:53.644 --> 00:01:55.939
recall that we want to train the discriminator to

00:01:55.939 --> 00:01:59.829
output values near one for real data and near-zero for fake data.

00:01:59.829 --> 00:02:05.060
This is just like training irregular classifier in terms of the loss function we'll use.

00:02:05.060 --> 00:02:09.425
A lot of the time, people make mistakes trying to implement their own loss function,

00:02:09.425 --> 00:02:13.160
you want to remember that this is just a sigmoid cross entropy loss like you've

00:02:13.159 --> 00:02:17.150
seen before and implemented the same way as in other classifiers.

00:02:17.150 --> 00:02:20.719
One place where people often make a mistake is they forget to use

00:02:20.719 --> 00:02:22.219
the numerically stable version of

00:02:22.219 --> 00:02:25.805
cross-entropy where the loss is computed using the logits.

00:02:25.805 --> 00:02:30.784
The logits are the values produced by the discriminator right before the sigmoid.

00:02:30.784 --> 00:02:34.549
If you use the probability values that come out of the sigmoid,

00:02:34.550 --> 00:02:38.755
there can be problems with rounding error when the sigmoid is near zero or one.

00:02:38.754 --> 00:02:41.460
One GAN-specific trick is to multiply

00:02:41.460 --> 00:02:45.520
the zero or one labels by a number that is just a little bit smaller than one,

00:02:45.520 --> 00:02:48.094
so that you replace labels of one with labels of,

00:02:48.094 --> 00:02:52.254
for example, 0.9 and keep the zero labels at zero.

00:02:52.254 --> 00:02:54.229
This is GAN-specific version of

00:02:54.229 --> 00:02:58.069
the label smoothing strategy used to regularize normal classifiers.

00:02:58.069 --> 00:03:01.219
It helps the discriminator to generalize better and

00:03:01.219 --> 00:03:04.569
avoid learning to make extreme predictions when extrapolating.

00:03:04.569 --> 00:03:06.164
For the generator loss,

00:03:06.164 --> 00:03:08.634
you want to set up another cross-entropy loss,

00:03:08.634 --> 00:03:10.324
but with the labels flipped.

00:03:10.324 --> 00:03:15.219
In other words, the generator will maximize the log probability of the wrong labels.

00:03:15.219 --> 00:03:20.240
A lot of people like to use negative d-loss as the expression for g-loss.

00:03:20.240 --> 00:03:22.939
It's very intuitive because it corresponds to

00:03:22.939 --> 00:03:26.194
having the generator maximize the loss for the discriminator.

00:03:26.194 --> 00:03:30.514
We even use that loss for a visualization earlier in this course,

00:03:30.514 --> 00:03:32.958
but it doesn't work very well in practice

00:03:32.959 --> 00:03:35.270
and we don't actually recommend that you implement it for

00:03:35.270 --> 00:03:40.325
real because the gradient of d-loss is zero whenever the discriminator is winning.

00:03:40.324 --> 00:03:43.375
Using negative d-loss as the loss for g,

00:03:43.375 --> 00:03:46.504
forces that generator to maximize a cross-entropy.

00:03:46.504 --> 00:03:50.060
We really want both players to minimize the cross-entropy.

00:03:50.060 --> 00:03:54.469
Minimizing a cross-entropy works very well in practice because the derivative of

00:03:54.469 --> 00:03:56.550
the cross-entropy function is always

00:03:56.550 --> 00:03:59.705
non-zero whenever the cross entropy is not minimized.

00:03:59.705 --> 00:04:02.615
So, the losing player always gets gradient.

00:04:02.615 --> 00:04:04.730
That means it's much better for the generator to

00:04:04.729 --> 00:04:06.649
minimize the cross-entropy with the labels

00:04:06.650 --> 00:04:10.745
flipped rather than maximizing the discriminators' cross-entropy.

00:04:10.745 --> 00:04:14.180
There are lots of other options for d-loss and g-loss.

00:04:14.180 --> 00:04:18.725
New GAN papers are coming out constantly and many of them proposed new losses.

00:04:18.725 --> 00:04:22.715
You'll take some time to see which ones end up working the best for the most people.

00:04:22.714 --> 00:04:26.839
So far, everything we have discussed applies to almost every GAN.

00:04:26.839 --> 00:04:30.514
Now, let's look at what we need to do to scale GANs up to large images.

00:04:30.514 --> 00:04:33.574
To scale up classifiers to work on large images,

00:04:33.574 --> 00:04:35.629
we use convolutional networks.

00:04:35.629 --> 00:04:37.579
Convolutional networks replace some of

00:04:37.579 --> 00:04:41.209
the matrix multiplies in normal neural networks with convolutions.

00:04:41.209 --> 00:04:45.125
We can also replace the matrix multiplies in GANs with convolutions.

00:04:45.125 --> 00:04:50.029
For the input to the GAN generator we usually use a random vector Z.

00:04:50.029 --> 00:04:53.239
A mini-batch of these vectors forms a matrix.

00:04:53.240 --> 00:04:56.990
The problem is, a convolution expects its mini-batch to be

00:04:56.990 --> 00:05:01.519
a 4D Tensor with one axis for different examples in the mini-batch,

00:05:01.519 --> 00:05:04.099
one axis for the different feature maps,

00:05:04.100 --> 00:05:06.275
and then the width and height axes.

00:05:06.274 --> 00:05:08.964
To get the random input into this format,

00:05:08.964 --> 00:05:12.969
we have to use a reshape up near the start of that generator.

00:05:12.970 --> 00:05:17.765
For example, we might transform a mini-batch of vectors that each contain

00:05:17.764 --> 00:05:23.560
1,000 entries into a stack of 10 feature maps that are each sized 10 by 10.

00:05:23.560 --> 00:05:26.600
Usually, convolutional nets changed

00:05:26.600 --> 00:05:30.175
the shape of the feature maps as we move through the network.

00:05:30.175 --> 00:05:32.840
The input to a convolutional net is

00:05:32.839 --> 00:05:37.019
a very tall and wide image with just three feature maps: the red,

00:05:37.019 --> 00:05:39.310
green, and blue color channels.

00:05:39.310 --> 00:05:42.875
After applying convolution and pooling several times,

00:05:42.875 --> 00:05:46.564
we end up with very short and narrow feature maps.

00:05:46.564 --> 00:05:51.064
That's all what happens when we use a classifier convolutional net.

00:05:51.064 --> 00:05:54.439
When we use a convolutional net as a generator net,

00:05:54.439 --> 00:05:57.634
most researchers think we probably want to do the opposite.

00:05:57.634 --> 00:06:03.185
We want to start off with a small feature map and expand it to a wide and tall image.

00:06:03.185 --> 00:06:06.064
To do this, we need to have an op that can

00:06:06.064 --> 00:06:09.545
increase the width and the height of feature maps at every layer.

00:06:09.545 --> 00:06:14.330
The DCGAN Project introduced the idea of increasing the size of

00:06:14.329 --> 00:06:19.914
the feature maps just by using convolution transpose ops with stride greater than one.

00:06:19.915 --> 00:06:22.754
This just means that when we compute the convolution,

00:06:22.754 --> 00:06:27.379
every time we move the convolution kernel by one pixel in the input map we move

00:06:27.379 --> 00:06:32.170
it by two pixels or some other larger number of pixels in the output map.

00:06:32.170 --> 00:06:36.530
Finally, you really want to use batch normalization or one of

00:06:36.529 --> 00:06:41.259
the many follow-up methods based on batch normalization in most layers of the network.

00:06:41.259 --> 00:06:45.034
The DCGAN authors recommend using batch normalization on

00:06:45.035 --> 00:06:46.640
every layer except the output layer of

00:06:46.639 --> 00:06:49.579
the generator and the input layer of the discriminator.

00:06:49.579 --> 00:06:53.209
They also apply batch norm to all the real data in one mini-batch.

00:06:53.209 --> 00:06:55.234
Then, apply batch normalization

00:06:55.235 --> 00:06:59.405
separately to another mini-batch containing all the generated samples.

00:06:59.404 --> 00:07:03.469
That's a little bit weird from a theoretical point of view because it means we use

00:07:03.470 --> 00:07:06.140
a different discriminator function based on

00:07:06.139 --> 00:07:10.669
different batch statistics for real data than we use for generated data.

00:07:10.670 --> 00:07:12.379
Despite being a little bit weird,

00:07:12.379 --> 00:07:14.435
it seems to work well in practice

00:07:14.435 --> 00:07:17.329
You can read about a few different alternative normalization

00:07:17.329 --> 00:07:20.495
strategies that are more theoretically straightforward

00:07:20.495 --> 00:07:24.415
in an open AI paper called Improved Techniques for Training GANs.

00:07:24.415 --> 00:07:27.754
Overall, this recipe of using convolution transpose,

00:07:27.754 --> 00:07:30.019
batch normalization, Adam and

00:07:30.019 --> 00:07:34.529
cross-entropy losses with label smoothing works fairly well in practice.

